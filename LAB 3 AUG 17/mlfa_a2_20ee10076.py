# # -*- coding: utf-8 -*-
# """MLFA_A2_20EE10076.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1DLaTxX-i3ZjqZSeVFoQ0k6QRLkihKEEi
# """

#NAME: Susnata Biswas | Roll No. 20EE10076 | MLFA Assignment- 02

import numpy as np
import pandas as pd

df = pd.read_csv('dataset.csv')
df

df.dtypes

df.describe(include='all').T

df

df.drop(['Product_ID'], axis = 1, inplace = True)

df.User_ID.nunique()

df

def barplot_with_percentage(data,x_label,y_label,figsize=(9,5)):
    plt.figure(figsize=figsize)
    g = sns.barplot(data,x=x_label,y=y_label)
    g.set_xticklabels(labels=data[x_label].to_list(), rotation=90)

    for p in g.patches:
        txt = str(p.get_height().round(2)) + '%'
        txt_x = p.get_x()
        txt_y = p.get_height() + 0.1
        g.text(txt_x,txt_y,txt)

    plt.show()

df.Age.value_counts(normalize=True).head()

df.Occupation.value_counts(normalize=True).head()

df.City_Category.value_counts(normalize=True).head()

df.Stay_In_Current_City_Years.value_counts(normalize=True).head()

df.Product_Category_1.value_counts(normalize=True).head()

df.Product_Category_2.value_counts(normalize=True).head()

df.Product_Category_3.value_counts(normalize=True).head()

df

df.isnull().sum()

df.fillna(df.mean(), inplace=True)
df

df.isnull().sum()

df

df['Gender'].replace({'M': 1, 'F': 0}, inplace=True)
df['City_Category'].replace({'A': 1, 'B': 2, 'C': 3}, inplace=True)

df

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

matrix = df.corr().round(2)
sns.heatmap(matrix, annot=True)
plt.show()

correlation_matrix = df.corr()

columns_to_drop = ['User_ID', 'Occupation', 'Marital_Status', 'Age', 'City_Category', 'Stay_In_Current_City_Years']

data_filtered = df.drop(columns=columns_to_drop)

data_filtered

import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.metrics import mean_squared_error

X = data_filtered.drop('Purchase', axis=1)
y = data_filtered['Purchase']

# Perform an 80-20 train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train_with_bias = X_train.copy()
X_test_with_bias = X_test.copy()

# Calculate the closed-form solution coefficients using X_train and y_train
X_train_transpose = X_train_with_bias.transpose()
X_train_transpose_X_train = np.dot(X_train_transpose, X_train_with_bias)
X_train_transpose_X_train_inv = np.linalg.inv(X_train_transpose_X_train)
X_train_transpose_y_train = np.dot(X_train_transpose, y_train)
theta = np.dot(X_train_transpose_X_train_inv, X_train_transpose_y_train)

y_pred = np.dot(X_test_with_bias, theta)

# Calculate Mean Squared Error (MSE) to evaluate model accuracy
mse = mean_squared_error(y_test, y_pred)
print("Model MSE with Preprocessing:, mse")

X_train

X_train

X_train_array = X_train.values
X_test_array = X_test.values
y_train_array = y_train.values

# Learning rate values to try
learning_rates = [1e-5, 1e-4, 1e-3, 1e-2, 0.05, 0.1]
batch_size = 32
num_epochs = 10

# Initialize coefficients randomly
num_features = X_train_array.shape[1]
theta_initial = np.random.rand(num_features)

# Dictionary to store MSE values for different learning rates
mse_values = []

# Gradient Descent with Mini-Batch for different learning rates
for learning_rate in learning_rates:
    theta = np.copy(theta_initial)  # Reset theta to initial values
    mse_epochs = []

    for epoch in range(num_epochs):
        for batch_start in range(0, len(X_train_array), batch_size):
            batch_end = batch_start + batch_size
            X_batch = X_train_array[batch_start:batch_end]
            y_batch = y_train_array[batch_start:batch_end]

            # Calculate predictions and errors
            y_pred = np.dot(X_batch, theta)
            errors = y_pred - y_batch

            # Calculate gradients and update coefficients
            gradients = np.dot(X_batch.T, errors) / batch_size
            theta -= learning_rate * gradients

        # Calculate MSE for the current epoch
        y_pred_test = np.dot(X_test_array, theta)
        mse_epochs.append(mean_squared_error(y_test, y_pred_test))

    # mse_values[learning_rate] = mse_epochs

# Plotting MSE vs Learning Rate
plt.figure(figsize=(10, 6))
for learning_rate, mse_epochs in mse_values.items():
    plt.plot(range(num_epochs), mse_epochs, label=f'Learning Rate: {learning_rate}')
plt.xlabel('Epoch')
plt.ylabel('Mean Squared Error (MSE)')
plt.title('MSE vs Learning Rate')
plt.legend()
plt.show()

# Find the index of the alpha with the lowest MSE
best_alpha_index = np.argmin(mse_values)
best_alpha = learning_rates[best_alpha_index]
print("Best Alpha:", best_alpha)

from sklearn.linear_model import Ridge


# Perform an 80-20 train-validation split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize alpha values
alpha_values = np.arange(0.0, 1.1, 0.1)

# Initialize a list to store MSE values
mse_values = []

# Loop through different alpha values
for alpha in alpha_values:
    # Initialize and train the Ridge Regression model
    ridge_model = Ridge(alpha=alpha)
    ridge_model.fit(X_train, y_train)

    # Predict on the validation set
    y_pred = ridge_model.predict(X_val)

    # Calculate MSE and store in the list
    mse = mean_squared_error(y_val, y_pred)
    mse_values.append(mse)

# Plotting MSE vs Alpha
plt.figure(figsize=(10, 6))
plt.plot(alpha_values, mse_values, marker='o', linestyle='-', color='b')
plt.xlabel('Alpha (Regularization Strength)')
plt.ylabel('Mean Squared Error (MSE)')
plt.title('MSE vs Alpha')
plt.grid(True)
plt.show()

# Find the index of the alpha with the lowest MSE
best_alpha_index = np.argmin(mse_values)
best_alpha = alpha_values[best_alpha_index]
print("Best Alpha:", best_alpha)

# Find the index of the alpha with the lowest MSE
best_alpha_index = np.argmin(mse_values)
best_alpha = alpha_values[best_alpha_index]
print("Best Alpha:", best_alpha)